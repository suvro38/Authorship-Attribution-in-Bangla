{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and data\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sbnltk.Tokenizer import sentenceTokenizer, wordTokenizer\n",
    "\n",
    "# read training data\n",
    "data = pd.read_csv('data/macroTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TTR\n",
    "def calculate_ttr(text):\n",
    "    \"\"\"\n",
    "    Calculate the Type-Token Ratio (TTR) for a given text.\n",
    "    Input:\n",
    "    - text: str, the input text\n",
    "    Output:\n",
    "    - ttr: float, the Type-Token Ratio\n",
    "    \"\"\"\n",
    "    tokenizer = wordTokenizer()  # Initialize word tokenizer\n",
    "    tokens = tokenizer.customized_tokenizer(text)  # Tokenize the text\n",
    "    types = set(tokens)  # Get unique tokens\n",
    "    ttr = len(types) / len(tokens)  # Calculate TTR\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create character bigram model using MLE and Add-One smoothing\n",
    "def create_char_bigram(texts):\n",
    "     \"\"\"\n",
    "    Create a character bigram model using Maximum Likelihood Estimate (MLE) and Add-One smoothing.\n",
    "    Input:\n",
    "    - texts: list of str, a list of texts\n",
    "    Output:\n",
    "    - char_bigram_model: Counter, the character bigram model\"\"\"\n",
    "    char_bigram_model = Counter()  # Initialize character bigram model\n",
    "    \n",
    "    for text in texts:\n",
    "        char_bigrams = [text[i:i+2] for i in range(len(text) - 1)]  # Generate character bigrams\n",
    "        char_bigram_counts = Counter(char_bigrams)  # Count the occurrences of each bigram\n",
    "        \n",
    "        v = len(set(text))  # V = number of unique characters\n",
    "        \n",
    "        for bigram, count in char_bigram_counts.items():\n",
    "            # Calculate Maximum Likelihood Estimate (MLE) probability\n",
    "            wi_minus_1 = bigram[0]  # First character of the bigram\n",
    "            probability = (count + 1) / (text.count(wi_minus_1) + v)  # Add-One smoothing formula\n",
    "            \n",
    "            char_bigram_model[bigram] += probability  # assign probability to the bigram\n",
    "    \n",
    "    return char_bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word bigram model with Add-One smoothing\n",
    "def create_word_bigram(author_texts):\n",
    "    \"\"\"\n",
    "    Create a word bigram model with Add-One smoothing.\n",
    "    Input:\n",
    "    - author_texts: list of str, texts authored by a specific author\n",
    "    Output:\n",
    "    - word_bigram_model: Counter, the word bigram model\n",
    "    \"\"\"\n",
    "    word_bigram_model = Counter()  # Initialize word bigram model\n",
    "    \n",
    "    sentT = sentenceTokenizer()  # Initialize sentence tokenizer\n",
    "    Wt = wordTokenizer()  # Initialize word tokenizer\n",
    "    \n",
    "    all_words = []  # List to store all words in all texts of the author\n",
    "    \n",
    "    for text in author_texts:\n",
    "        sentences = sentT.customized_tokenizer(text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = ['<s>'] + Wt.customized_tokenizer(sentence) + ['</s>']\n",
    "            \n",
    "            # Remove specific bigrams\n",
    "            words = [word for word in words if word not in [('', '</s>'), ('</s>', '<s>'), ('<s>', '')]]\n",
    "            \n",
    "            all_words.extend(words)  # Add words to the list of all words\n",
    "    \n",
    "    v = len(set(all_words))\n",
    "    \n",
    "    for i in range(len(all_words) - 1):\n",
    "        bigram = (all_words[i], all_words[i+1])  # Get current bigram\n",
    "        \n",
    "        # Count occurrences of the bigram and the preceding word\n",
    "        count_bigram = all_words.count(bigram)\n",
    "        count_wi_minus_1 = all_words.count(bigram[0])\n",
    "        \n",
    "        # Add-One smoothing\n",
    "        probability = (count_bigram + 1) / (count_wi_minus_1 + v)  # Add-One smoothing formula\n",
    "        \n",
    "        # probability to the bigram in the model\n",
    "        word_bigram_model[bigram] += probability \n",
    "    \n",
    "    return word_bigram_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for each author\n",
    "authors = data['label'].unique()\n",
    "author_ttr_models = {}\n",
    "author_char_bigram_models = {}\n",
    "author_word_bigram_models = {}\n",
    "\n",
    "for author in authors:\n",
    "    author_data = data[data['label'] == author]\n",
    "    author_texts = author_data['text']\n",
    "    \n",
    "    author_ttr_models[author] = np.mean(author_texts.apply(calculate_ttr))  # Average TTR for the author\n",
    "    \n",
    "    char_bigram_model = create_char_bigram(author_texts)\n",
    "    word_bigram_model = create_word_bigram(author_texts)\n",
    "    \n",
    "    author_char_bigram_models[author] = char_bigram_model  # Store the character bigram model\n",
    "    author_word_bigram_models[author] = word_bigram_model  # Store the word bigram model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict author using TTR\n",
    "def predict_author_ttr(test_text, author_ttr_models):\n",
    "    \"\"\"\n",
    "    Predict the author of a given text using Type-Token Ratio (TTR) models.\n",
    "    Input:\n",
    "    - test_text: str, the test text\n",
    "    - author_ttr_models: dict, TTR models for each author\n",
    "    Output:\n",
    "    - predicted_author: str, the predicted author\n",
    "    \"\"\"\n",
    "    test_ttr = calculate_ttr(test_text)  # Calculate TTR for test text\n",
    "    \n",
    "    author_scores = {}\n",
    "    for author, ttr_model in author_ttr_models.items():\n",
    "        author_scores[author] = abs(test_ttr - ttr_model)  # Calculate difference in TTR\n",
    "    \n",
    "    predicted_author = min(author_scores, key=author_scores.get)  # Identify the author with the lowest score\n",
    "    return predicted_author\n",
    "\n",
    "# Predict author using Character Bigram Model\n",
    "def predict_author_char_bigram(test_text, author_char_bigram_models):\n",
    "    \"\"\"\n",
    "    Predict the author of a given text using character bigram models.\n",
    "    Input:\n",
    "    - test_text: str, the test text\n",
    "    - author_char_bigram_models: dict, character bigram models for each author\n",
    "    Output:\n",
    "    - predicted_author: str, the predicted author\n",
    "    \"\"\"\n",
    "    test_char_bigram_model = create_char_bigram([test_text])  # Calculate character bigram model for test text\n",
    "    \n",
    "    author_scores = {}\n",
    "    for author, char_bigram_model in author_char_bigram_models.items():\n",
    "        author_scores[author] = 0\n",
    "        \n",
    "        # Compare character bigram models\n",
    "        for bigram, prob in test_char_bigram_model.items():\n",
    "            author_scores[author] += abs(prob - char_bigram_model.get(bigram, 0))\n",
    "    \n",
    "    predicted_author = min(author_scores, key=author_scores.get)  # Identify the author with the lowest score\n",
    "    return predicted_author\n",
    "\n",
    "# predict author using Word Bigram Model\n",
    "def predict_author_word_bigram(test_text, author_word_bigram_models):\n",
    "    \"\"\"\n",
    "    Predict the author of a given text using word bigram models.\n",
    "    Input:\n",
    "    - test_text: str, the test text\n",
    "    - author_word_bigram_models: dict, word bigram models for each author\n",
    "    Output:\n",
    "    - predicted_author: str, the predicted author\n",
    "    \"\"\"\n",
    "    test_word_bigram_model = create_word_bigram([test_text])  # Calculate word bigram model for test text\n",
    "    \n",
    "    author_scores = {}\n",
    "    for author, word_bigram_model in author_word_bigram_models.items():\n",
    "        author_scores[author] = 0\n",
    "        \n",
    "        # Compare word bigram models\n",
    "        for bigram, prob in test_word_bigram_model.items():\n",
    "            author_scores[author] += abs(prob - word_bigram_model.get(bigram, 0))\n",
    "    \n",
    "    predicted_author = min(author_scores, key=author_scores.get)  # Identify the author with the lowest score\n",
    "    return predicted_author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "data = pd.read_csv('macroTest.csv')\n",
    "\n",
    "# variables to count matches for each model\n",
    "match_ttr = 0\n",
    "match_char_bigram = 0\n",
    "match_word_bigram = 0\n",
    "\n",
    "# Iterate over each row in the test data\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    actual_author = row['label']    # actual author\n",
    "\n",
    "    test_text = row['text']     # text\n",
    "    \n",
    "    # Predict author using TTR\n",
    "    predicted_author_ttr = predict_author_ttr(test_text, author_ttr_models)\n",
    "    \n",
    "    # Predict author using Character Bigram Model\n",
    "    predicted_author_char_bigram = predict_author_char_bigram(test_text, author_char_bigram_models)\n",
    "    \n",
    "    # Predict author using Word Bigram Model\n",
    "    predicted_author_word_bigram = predict_author_word_bigram(test_text, author_word_bigram_models)\n",
    "    \n",
    "    # Check if predictions match the actual author and update match counts\n",
    "    if predicted_author_ttr == actual_author:\n",
    "        match_ttr += 1\n",
    "    if predicted_author_char_bigram == actual_author:\n",
    "        match_char_bigram += 1\n",
    "    if predicted_author_word_bigram == actual_author:\n",
    "        match_word_bigram += 1\n",
    "\n",
    "# Calculate accuracy scores\n",
    "total_texts = len(data)\n",
    "accuracy_ttr = match_ttr / total_texts\n",
    "accuracy_char_bigram = match_char_bigram / total_texts\n",
    "accuracy_word_bigram = match_word_bigram / total_texts\n",
    "\n",
    "# Output results\n",
    "print(\"Accuracy using TTR:\", accuracy_ttr)\n",
    "print(\"Accuracy using Character Bigram Model:\", accuracy_char_bigram)\n",
    "print(\"Accuracy using Word Bigram Model:\", accuracy_word_bigram)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
